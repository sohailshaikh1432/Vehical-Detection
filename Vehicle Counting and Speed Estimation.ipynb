{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Counting and Speed Estimation\n",
    "- https://github.com/iandees/speedtrack/blob/master/record.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "#\n",
    "# Tracks cars out my window using OpenCV.\n",
    "#\n",
    "# This works by keeping track of a running average for the scene and\n",
    "# subtracting the average from the current frame to find the parts\n",
    "# that are different/moving (like cars). The difference is processed\n",
    "# to find the bounding box of these car-sized changes.\n",
    "#\n",
    "# Once the blobs are found, they are compared with previously-found\n",
    "# blobs so that we can track the progress of blobs across the image.\n",
    "# From those tracks we can compute speed and also count the number\n",
    "# of cars crossing the field of view in each direction.\n",
    "#\n",
    "\n",
    "# The cutoff for threshold. A lower number means smaller changes between\n",
    "# the average and current scene are more readily detected.\n",
    "THRESHOLD_SENSITIVITY = 50\n",
    "# Number of pixels in each direction to blur the difference between\n",
    "# average and current scene. This helps make small differences larger\n",
    "# and more detectable.\n",
    "BLUR_SIZE = 40\n",
    "# The number of square pixels a blob must be before we consider it a\n",
    "# candidate for tracking.\n",
    "BLOB_SIZE = 500\n",
    "# The number of pixels wide a blob must be before we consider it a\n",
    "# candidate for tracking.\n",
    "BLOB_WIDTH = 60\n",
    "# The weighting to apply to \"this\" frame when averaging. A higher number\n",
    "# here means that the average scene will pick up changes more readily,\n",
    "# thus making the difference between average and current scenes smaller.\n",
    "DEFAULT_AVERAGE_WEIGHT = 0.04\n",
    "# The maximum distance a blob centroid is allowed to move in order to\n",
    "# consider it a match to a previous scene's blob.\n",
    "BLOB_LOCKON_DISTANCE_PX = 80\n",
    "# The number of seconds a blob is allowed to sit around without having\n",
    "# any new blobs matching it.\n",
    "BLOB_TRACK_TIMEOUT = 0.7\n",
    "# The left and right X positions of the \"poles\". These are used to\n",
    "# track the speed of a vehicle across the scene.\n",
    "LEFT_POLE_PX = 320\n",
    "RIGHT_POLE_PX = 500\n",
    "# Constants for drawing on the frame.\n",
    "LINE_THICKNESS = 1\n",
    "CIRCLE_SIZE = 5\n",
    "RESIZE_RATIO = 0.4\n",
    "\n",
    "## Set up a video capture device (number for webcam, filename for video file input)\n",
    "# vc = cv2.VideoCapture(0)\n",
    "vc = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "def nothing(*args, **kwargs):\n",
    "    \" A helper function to use for OpenCV slider windows. \"\n",
    "    print(args, kwargs)\n",
    "\n",
    "def get_frame():\n",
    "    \" Grabs a frame from the video capture and resizes it. \"\n",
    "    rval, frame = vc.read()\n",
    "    if rval:\n",
    "        (h, w) = frame.shape[:2]\n",
    "        frame = cv2.resize(frame, (int(w * RESIZE_RATIO), int(h * RESIZE_RATIO)), interpolation=cv2.INTER_CUBIC)\n",
    "    return rval, frame\n",
    "\n",
    "from itertools import tee\n",
    "#from itertools import zip\n",
    "from itertools import zip_longest as zip\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "# cv2.namedWindow(\"preview\")\n",
    "# cv2.cv.SetMouseCallback(\"preview\", nothing)\n",
    "\n",
    "# A variable to store the running average.\n",
    "avg = None\n",
    "# A list of \"tracked blobs\".\n",
    "tracked_blobs = []\n",
    "\n",
    "while True:\n",
    "    # Grab the next frame from the camera or video file\n",
    "    grabbed, frame = get_frame()\n",
    "\n",
    "    if not grabbed:\n",
    "        # If we fall into here it's because we ran out of frames\n",
    "        # in the video file.\n",
    "        break\n",
    "\n",
    "    frame_time = time.time()\n",
    "\n",
    "    # Convert the frame to Hue Saturation Value (HSV) color space.\n",
    "    hsvFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    # Only use the Value channel of the frame.\n",
    "    (_, _, grayFrame) = cv2.split(hsvFrame)\n",
    "    # Apply a blur to the frame to smooth out any instantaneous changes\n",
    "    # like leaves glinting in sun or birds flying around.\n",
    "    grayFrame = cv2.GaussianBlur(grayFrame, (21, 21), 0)\n",
    "\n",
    "    if avg is None:\n",
    "        # Set up the average if this is the first time through.\n",
    "        avg = grayFrame.copy().astype(\"float\")\n",
    "        continue\n",
    "\n",
    "    # Build the average scene image by accumulating this frame\n",
    "    # with the existing average.\n",
    "    cv2.accumulateWeighted(grayFrame, avg, DEFAULT_AVERAGE_WEIGHT)\n",
    "    cv2.imshow(\"average\", cv2.convertScaleAbs(avg))\n",
    "\n",
    "    # Compute the grayscale difference between the current grayscale frame and\n",
    "    # the average of the scene.\n",
    "    differenceFrame = cv2.absdiff(grayFrame, cv2.convertScaleAbs(avg))\n",
    "    cv2.imshow(\"difference\", differenceFrame)\n",
    "\n",
    "    # Apply a threshold to the difference: any pixel value above the sensitivity\n",
    "    # value will be set to 255 and any pixel value below will be set to 0.\n",
    "    retval, thresholdImage = cv2.threshold(differenceFrame, THRESHOLD_SENSITIVITY, 255, cv2.THRESH_BINARY)\n",
    "    thresholdImage = cv2.dilate(thresholdImage, None, iterations=2)\n",
    "    cv2.imshow(\"threshold\", thresholdImage)\n",
    "\n",
    "    # Find contours aka blobs in the threshold image.\n",
    "    contours, hierarchy = cv2.findContours(thresholdImage, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter out the blobs that are too small to be considered cars.\n",
    "    blobs = filter(lambda c: cv2.contourArea(c) > BLOB_SIZE, contours)\n",
    "\n",
    "    if blobs:\n",
    "        for c in blobs:\n",
    "            # Find the bounding rectangle and center for each blob\n",
    "            (x, y, w, h) = cv2.boundingRect(c)\n",
    "            center = (int(x + w/2), int(y + h/2))\n",
    "\n",
    "            ## Optionally draw the rectangle around the blob on the frame that we'll show in a UI later\n",
    "            # cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), LINE_THICKNESS)\n",
    "\n",
    "            # Look for existing blobs that match this one\n",
    "            closest_blob = None\n",
    "            if tracked_blobs:\n",
    "                # Sort the blobs we have seen in previous frames by pixel distance from this one\n",
    "                closest_blobs = sorted(tracked_blobs, key=lambda b: cv2.norm(b['trail'][0], center))\n",
    "\n",
    "                # Starting from the closest blob, make sure the blob in question is in the expected direction\n",
    "                for close_blob in closest_blobs:\n",
    "                    distance = cv2.norm(center, close_blob['trail'][0])\n",
    "\n",
    "                    # Check if the distance is close enough to \"lock on\"\n",
    "                    if distance < BLOB_LOCKON_DISTANCE_PX:\n",
    "                        # If it's close enough, make sure the blob was moving in the expected direction\n",
    "                        expected_dir = close_blob['dir']\n",
    "                        if expected_dir == 'left' and close_blob['trail'][0][0] < center[0]:\n",
    "                            continue\n",
    "                        elif expected_dir == 'right' and close_blob['trail'][0][0] > center[0]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            closest_blob = close_blob\n",
    "                            break\n",
    "\n",
    "                if closest_blob:\n",
    "                    # If we found a blob to attach this blob to, we should\n",
    "                    # do some math to help us with speed detection\n",
    "                    prev_center = closest_blob['trail'][0]\n",
    "                    if center[0] < prev_center[0]:\n",
    "                        # It's moving left\n",
    "                        closest_blob['dir'] = 'left'\n",
    "                        closest_blob['bumper_x'] = x\n",
    "                    else:\n",
    "                        # It's moving right\n",
    "                        closest_blob['dir'] = 'right'\n",
    "                        closest_blob['bumper_x'] = x + w\n",
    "\n",
    "                    # ...and we should add this centroid to the trail of\n",
    "                    # points that make up this blob's history.\n",
    "                    closest_blob['trail'].insert(0, center)\n",
    "                    closest_blob['last_seen'] = frame_time\n",
    "\n",
    "            if not closest_blob:\n",
    "                # If we didn't find a blob, let's make a new one and add it to the list\n",
    "                b = dict(\n",
    "                    id=str(uuid.uuid4())[:8],\n",
    "                    first_seen=frame_time,\n",
    "                    last_seen=frame_time,\n",
    "                    dir=None,\n",
    "                    bumper_x=None,\n",
    "                    trail=[center],\n",
    "                )\n",
    "                tracked_blobs.append(b)\n",
    "\n",
    "    if tracked_blobs:\n",
    "        # Prune out the blobs that haven't been seen in some amount of time\n",
    "        for i in range(len(tracked_blobs) - 1, -1, -1):\n",
    "            if frame_time - tracked_blobs[i]['last_seen'] > BLOB_TRACK_TIMEOUT:\n",
    "                print(\"Removing expired track {}\".format(tracked_blobs[i]['id']))\n",
    "                del tracked_blobs[i]\n",
    "\n",
    "    # Draw the fences\n",
    "    # cv2.line(frame, (LEFT_POLE_PX, 0), (LEFT_POLE_PX, 700), (100, 100, 100), 2)\n",
    "    # cv2.line(frame, (RIGHT_POLE_PX, 0), (RIGHT_POLE_PX, 700), (100, 100, 100), 2)\n",
    "\n",
    "    # Draw information about the blobs on the screen\n",
    "    for blob in tracked_blobs:\n",
    "        for (a, b) in pairwise(blob['trail']):\n",
    "            cv2.circle(frame, a, 3, (255, 0, 0), LINE_THICKNESS)\n",
    "\n",
    "            if blob['dir'] == 'left':\n",
    "                cv2.line(frame, a, b, (255, 255, 0), LINE_THICKNESS)\n",
    "            else:\n",
    "                cv2.line(frame, a, b, (0, 255, 255), LINE_THICKNESS)\n",
    "\n",
    "            bumper_x = blob['bumper_x']\n",
    "            if bumper_x:\n",
    "                cv2.line(frame, (bumper_x, 100), (bumper_x, 500), (255, 0, 255), 3)\n",
    "\n",
    "            # cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), LINE_THICKNESS)\n",
    "            # cv2.circle(frame, center, 10, (0, 255, 0), LINE_THICKNESS)\n",
    "\n",
    "    # Show the image from the camera (along with all the lines and annotations)\n",
    "    # in a window on the user's screen.\n",
    "    cv2.imshow(\"preview\", frame)\n",
    "\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 27: # exit on ESC\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fa156b34304f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# Continuously grab a frame and write it to the outputter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mgrabbed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-fa156b34304f>\u001b[0m in \u001b[0;36mget_frame\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;34m\"\"\" Helper function to grab a frame from the webcam, scale it, and return. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# (h, w) = frame.shape[:2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# frame = cv2.resize(frame, (int(w * RESIZE_RATIO), int(h * RESIZE_RATIO)), interpolation=cv2.INTER_CUBIC)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "#\n",
    "# Records a webcam feed to an MP4 file using OpenCV.\n",
    "#\n",
    "# This is handy to record input for other CV programs without\n",
    "# worrying too much about variable lighting conditions.\n",
    "#\n",
    "\n",
    "# Opens the default webcam device\n",
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "def get_frame():\n",
    "    \"\"\" Helper function to grab a frame from the webcam, scale it, and return. \"\"\"\n",
    "    rval, frame = vc.read()\n",
    "    # (h, w) = frame.shape[:2]\n",
    "    # frame = cv2.resize(frame, (int(w * RESIZE_RATIO), int(h * RESIZE_RATIO)), interpolation=cv2.INTER_CUBIC)\n",
    "    return rval, frame\n",
    "\n",
    "# Grab a single frame from the camera. `grabbed` will be false\n",
    "# if something failed, `frame` is an array of data containing\n",
    "# the image.\n",
    "grabbed, frame = get_frame()\n",
    "\n",
    "# Specify the output encoding for the saved file\n",
    "#fourcc = cv2.cv.CV_FOURCC(*'mp4v')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "# Get the size of the frame from the camera\n",
    "#(h, w) = frame.shape[:2]\n",
    "\n",
    "import cv2\n",
    "\n",
    "vcap = cv2.VideoCapture('video.mp4') # 0=camera\n",
    "\n",
    "if vcap.isOpened(): \n",
    "    # get vcap property \n",
    "    #width = vcap.get(cv2.cv.CV_CAP_PROP_FRAME_WIDTH)   # float\n",
    "    #height = vcap.get(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT) # float\n",
    "\n",
    "    # or\n",
    "    width = vcap.get(3)  # float\n",
    "    height = vcap.get(4) # float\n",
    "\n",
    "    # it gives me 0.0 :/\n",
    "    #fps = vcap.get(cv2.CV_CAP_PROP_FPS)\n",
    "    fps = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Build a video writer that writes to a file name with the specified\n",
    "# output encoding, framerate, and size\n",
    "writer = cv2.VideoWriter('output.mp4', fourcc, 40.0, (30, 40))\n",
    "\n",
    "while True:\n",
    "    # Continuously grab a frame and write it to the outputter\n",
    "    grabbed, frame = get_frame()\n",
    "    writer.write(frame)\n",
    "\n",
    "# Ideally we would listen for a signal and clean up the\n",
    "# camera and writer here, but it seems to work without.\n",
    "#© 2019 GitHub, Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "#https://stackoverflow.com/questions/48251545/module-cv2-cv2-has-no-attribute-cv\n",
    "import cv2\n",
    "\n",
    "## opening videocapture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "## some videowriter props\n",
    "sz = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "fps = 20\n",
    "#fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "#fourcc = cv2.VideoWriter_fourcc('m', 'p', 'e', 'g')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mpeg')\n",
    "\n",
    "## open and set props\n",
    "vout = cv2.VideoWriter()\n",
    "vout.open('output.mp4',fourcc,fps,sz,True)\n",
    "\n",
    "cnt = 0\n",
    "while cnt<20:\n",
    "    cnt += 1\n",
    "    print(cnt)\n",
    "    _, frame = cap.read()\n",
    "    cv2.putText(frame, str(cnt), (10, 20), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,0), 1, cv2.LINE_AA)\n",
    "    vout.write(frame)\n",
    "\n",
    "vout.release()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
